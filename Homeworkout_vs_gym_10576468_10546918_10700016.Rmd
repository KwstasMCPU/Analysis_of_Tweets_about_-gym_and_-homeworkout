---
title: 'MATH513   Big Data and Social Network Visualization   Practical #Homeworkout
  vs #Gym '
author: '"10576468, 10546918, 10700016"'
date: "`r Sys.Date()`" #Returns the current system date.
output: beamer_presentation
header-includes:
  \usepackage{graphicx}
  \usepackage{fancyhdr}
  \addtolength{\headheight}{10pt} 
  \pagestyle{fancy} 
  \lhead{\includegraphics[width=1cm]{/Users/sidharthakoneru/Downloads/Rmarkdown/logo.jpg}}
  \renewcommand{\headrulewidth}{0pt}
---

```{r, echo=FALSE}
logo <-"logo.jpg"
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r twitter_request, eval = FALSE, include = FALSE}
# making request to the tweeter API
my_token <- create_token(
  app = "xxx",
  consumer_key = "xxxxx",
  consumer_secret = "xxxxx",
  access_token = "xxxxxx",
  access_secret = "xxxxxxx")
###########

# we filtered our search to receive tweets only with English language,  
# also we included r-tweets
hash_homeworkout_tweets <- search_tweets(q = "#homeworkout",
                        n = 1000, lang = "en", include_rts = TRUE)

hash_gym_tweets <- search_tweets(q = "#gym",
                                 n = 1000, lang = "en", include_rts = TRUE)
```

```{r, echo=FALSE, include=FALSE}
# loading libraries
library(rtweet)
library(ggplot2)
library(dplyr)
library(ggthemes)
library(readr)
library(jsonlite)
library(tidytext)
library(wordcloud)
library(wordcloud2)
library(tidyr)
library(maps)
library(scales)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#
#setting the working directory
setwd("xxxx")
#
# makes a dataframe out of a json file
# read the data from the .json files
hash_homeworkout_tweets <- stream_in(file("hash_homeworkout_tweets.json"))
hash_gym_tweets <- stream_in(file("hash_gym_tweets.json"))
#
```

```{r function_chunk, include=FALSE}
#
#creating functions
#
# Top locations
# 
plot_top_locations <- function(x, title_end = ' ', fill = 'blue') {x %>%
    count(location_rec, sort = TRUE) %>%
    mutate(location_rec = reorder(location_rec, n)) %>%
    na.omit() %>%
    head(10) %>%
    ggplot(aes(x = location_rec, y = n))+
    geom_col(fill = fill ) +
    coord_flip() +
    labs(x = "Location",
         y = "Number of tweets",
         title = paste("Top locations which tweeted:", title_end, sep = ' '),
         caption = 'Data Source: Twitter (derived using rtweet)') + 
    theme(axis.text = element_text(size = 16, color = "black"), 
          axis.title = element_text(size = 16, color = "black"),
          plot.title = element_text(size = 18, face = 'bold'),
          plot.caption = element_text(size = 11, face = 'italic'))
    }
#  
# Joining similar locations
#
join_similar_locations <- function(x){x %>%
    mutate(location_rec = 
             recode(location, 'United States' = 'USA', 'United State' = 'USA',
                    'US' = 'USA', 'Chicago' = 'Chicago, IL', 
                    "London, England" = "London",
                    "London,UK" = "London",
                    "London, UK" = "London", 
                    "South East, England" = 'United Kingdom',
                    "UK" = "United Kingdom", "u.k." = "United Kingdom",
                    "United Kingdom, EU" = "United Kingdom",
                    "England, United Kingdom" = "United Kingdom",
                    "united kingdom" = "United Kingdom",
                    "EU" = "European Union"
                    )
           )
  
    }
#
# function for the creation of a data frame with just the tweet texts, usernames and location data
#
create_lean_df <- function(df){ data.frame(date_time = df$created_at,
                                            username = df$screen_name,
                                            tweet_text = df$text,
                                            long = df$lng,
                                            lat = df$lat)
}
#
# function for ploting top negative and positive words
#
plot_bing_words <- function(df, title = '') {df %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(title = title,
       caption = 'Data Source: Twitter (derived using rtweet)',
       y = "Sentiment",
       x = NULL) +
  theme(axis.text = element_text(size = 14, color = "black"), 
        axis.title = element_text(size = 14, color = "black"),
        plot.title = element_text(size = 18, hjust = 0, face = 'bold'),
        plot.caption = element_text(size = 11, face = "italic"))
}
#
# function to create a df with the score of sentiments sentiments
#
sentiment_score <- function(x){ x %>%
    inner_join(get_sentiments("bing")) %>%
    count(tweetnumber, sentiment) %>%
    spread(sentiment, n, fill = 0) %>% # negative and positive sentiment in separate columns
    mutate(score = positive - negative) # the score is the sum of positive and negative appearances of each word in a tweet's text
}
#
# function for calculating the sentiment score means
#
sentiment_stats <- function(x){x %>%
    summarise(mean_score = mean(score),
              median_score = median(score),
              variance_score = var(score),
              sd_score = sd(score),
              IQR_score = IQR(score),
              min_score = min(score),
              max_score = max(score)
              )
}
```

## Comparison of Gyms and Home Workouts
![Image of gym.](gym.jpg){width=150px}  ![Image of home workout.](homeworkout.png){width=250px}
_Image (left): Cartoon doing barbells in gym, by Strelnikova, from: freepik.com_\
_Image (right): Cartoon doing lunges in home, by Macrovector, from: dreamstime.com_




## Type Of Tweets
```{r, echo = FALSE, message = FALSE, warning = FALSE, include = FALSE}
require(jsonlite)
require(dplyr)

hash_homeworkout_tweets <- stream_in(file("hash_homeworkout_tweets.json"))
hash_gym_tweets <- stream_in(file("hash_gym_tweets.json"))
hash_homeworkout_tweets_m <- hash_homeworkout_tweets %>%
  mutate(tweet = '#homeworkout')

hash_gym_tweets_m <- hash_gym_tweets%>%
  mutate(tweet = '#gym')

binded_tweets <- rbind(hash_homeworkout_tweets_m, hash_gym_tweets_m)
hash_tweets_organic<-binded_tweets[binded_tweets$is_retweet==FALSE,]

hash_tweets_organic<-subset(hash_tweets_organic,is.na(hash_tweets_organic$reply_to_status_id))
hash_tweets_retweet<- binded_tweets[binded_tweets$is_retweet==TRUE,]
hash_tweets_replies<- subset(binded_tweets,!is.na(binded_tweets$reply_to_status_id))

n_organic <- count(hash_tweets_organic)
n_retweets <- count(hash_tweets_retweet)
n_replies <- count(hash_tweets_replies)

data<- data.frame(category=c("Organic","Retweets","Replies"),count=c(as.numeric(n_organic), as.numeric(n_retweets), as.numeric(n_replies)))

data$fraction = data$count / sum(data$count)
data$percentage = signif(data$count /sum(data$count) *  100, 2)
data$ymax= cumsum(data$fraction)
data$ymin = c(0, head(data$ymax, n = -1))
type_of_tweet<- paste(data$category,data$percentage,"%")
```
```{r, echo = FALSE, message = FALSE, warning = FALSE}
ggplot(data,aes(ymax=ymax, ymin=ymin, xmax=4, xmin=3,fill=type_of_tweet)) +
  geom_rect() +
  coord_polar(theta = "y") +
  labs(fill="Type of tweet") +
  xlim(c(2,4)) +
  theme_void()+
  theme(legend.position = "right") +
  labs(caption = "Source: Data collected from Twitter's REST API via rtweet") +
  theme(plot.caption = element_text(size = 11, vjust = 1, face = 'italic'))
```


## Tweet Frequency 
```{r, eval = TRUE, include = FALSE, echo = FALSE, message = FALSE, warning = FALSE}
require(jsonlite)
rt_home <- stream_in(file("hash_homeworkout_tweets.json"))
```
```{r, echo = FALSE, message = FALSE, warning = FALSE}
require(ggplot2)
require(rtweet)
ts_plot(rt_home, "60 mins") +
  theme_minimal() + 
  theme(plot.title = element_text(face = "bold"),
        plot.caption = element_text(size = 11, face = 'italic')) + 
  labs(
    x = NULL, y = NULL, 
    title = "Frequency of #homeworkout Twitter statuses",
    subtitle = "Twitter status (tweet) counts aggregated using 1-hour intervals",
    caption = "Source: Data collected from Twitter's REST API via rtweet")
```

## Tweet Frequency 
```{r, eval = TRUE, include = FALSE, echo = FALSE, message = FALSE, warning = FALSE}
require(jsonlite)
rt_gym <- stream_in(file("hash_gym_tweets.json"))
```
```{r, echo = FALSE, message = FALSE, warning = FALSE}
require(ggplot2)
require(rtweet)

ts_plot(rt_gym, "60 mins") +
  theme_minimal() + 
  theme(plot.title = element_text(face = "bold"),
        plot.caption = element_text(size = 11, face = 'italic')) + 
  labs(
    x = NULL, y = NULL, 
    title = "Frequency of #gym Twitter statuses",
    subtitle = "Twitter status (tweet) counts aggregated using 1-hour intervals",
    caption = "Source: Data collected from Twitter's REST API via rtweet")
```


```{r, echo=FALSE}
#
# Cleaning data
#
# dealing with blank locations
#
hash_homeworkout_tweets$location[hash_homeworkout_tweets$location==""] <- NA
hash_gym_tweets$location[hash_gym_tweets$location==""] <- NA
#
# A location named "Nothing just happens." spotted. We clean it up since is not meaningful
#
hash_gym_tweets$location[hash_gym_tweets$location=="Nothing just happens."] <- NA
#
# Joining the similar locations with the join_similar_locations function we made previously
#
hash_homeworkout_tweets_recoded <- join_similar_locations(hash_homeworkout_tweets)
hash_gym_tweets_recoded <- join_similar_locations(hash_gym_tweets)
```
## Top Locations
```{r top_location_home, echo=FALSE, eval=FALSE}
# 
# Visuals for #homeworkout
# we did not included those graphs in the final presentation since we have the grid graph illustrating both
# there are included here for your reference and check that facet_grid graph work properly
plot_top_locations(hash_homeworkout_tweets_recoded, '#homeworkout', "tomato")
```
```{r top_location_gym, echo=FALSE, eval=FALSE}
#
# visuals for #gym
# we did not included those graphs in the final presentation since we have the grid graph illustrating both
plot_top_locations(hash_gym_tweets_recoded, '#gym', "steelblue")
length(hash_homeworkout_tweets$location[hash_homeworkout_tweets$location==NA])
length(hash_gym_tweets$location[hash_gym_tweets$location==NA])
```
```{r top_locations_facet_grid, echo=FALSE, message=FALSE, size=10}
#
# we bind the "hash_homeworkout_tweets" and "hash_gym_tweets" in order to create a facet_grid
#
# firstly we use mutate in order to create a column called tweet so we can 
# group_by them using their particular value (#homeworktout and #gym)
#
hash_homeworkout_tweets_m <- hash_homeworkout_tweets_recoded %>%
  mutate(tweet = '#homeworkout')
#
hash_gym_tweets_m <- hash_gym_tweets_recoded %>%
  mutate(tweet = '#gym')
#
# use rbind() to bind the together
#
binded_tweets <- rbind(hash_homeworkout_tweets_m, hash_gym_tweets_m)
#
# use the join_similar_locations function we made
#
binded_tweets_r <- join_similar_locations(binded_tweets)
#
# visual
#
binded_tweets_r %>%
  select(tweet, location_rec) %>% # just selecting the columns we need
  group_by(tweet) %>%
  count(location_rec) %>%
  na.omit() %>%
  arrange(desc(n)) %>%  # we faced a problem using head() with the binded data, 
  slice_head(n = 5) %>% # so instead we used arrange(desc) to sort the data, and then slice_head(5) to pick the first 5 results
  ggplot(aes(x = location_rec,y = n, fill = tweet)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL,
       y = "Number of tweets",
       fill = "Tweets",
       subtitle = "#homework vs #gym Tweets location",
       caption = "Source: Data collected from Twitter's REST API via rtweet") +
  theme(axis.text = element_text(size = 16, color = "black"),
        axis.title = element_text(size = 16, color = "black"),
        plot.subtitle = element_text(size= 17, face = 'bold'),
        legend.position = 'none',
        plot.caption = element_text(size = 11, face = 'italic')) +
  scale_fill_manual(values = c("#homeworkout" = "tomato", 
                               "#gym" = "steelblue")) +
  facet_grid(tweet~ ., scales ='free')
```

## World Base Map
```{r worldmap_pre_code, echo = FALSE, message=FALSE}
# 
# create variables indicating latitude and longitude using all available 
# tweet and profile geo-location data
#
hash_homeworkout_tweets_map <- lat_lng(hash_homeworkout_tweets)
hash_gym_tweets_map <- lat_lng(hash_gym_tweets)
#
# we create new data frame with just the tweet texts, user-names and location data
# using the create_lean_df function we made (see "function_chunk" chunk in the beginning of the script)
#
hash_homeworkout_tweets_map_s <- create_lean_df(hash_homeworkout_tweets_map)
#
hash_gym_tweets_map_s <- create_lean_df(hash_gym_tweets_map)
#
# remove na values
#
hash_homeworkout_locations  <- hash_homeworkout_tweets_map_s %>%
  na.omit()
#
hash_gym_locations  <- hash_gym_tweets_map_s %>%
  na.omit()
#
# round latitude and longitude and group close tweets
#
hash_homeworkout_locations_grp <- hash_homeworkout_locations %>%
  mutate(long_round = round(long, 2),
         lat_round = round(lat, 2)) %>%
  group_by(long_round, lat_round) %>%
  summarise(total_count = n()) %>%
  ungroup() 
#
hash_gym_locations_grp <- hash_gym_locations %>%
  mutate(long_round = round(long, 2),
         lat_round = round(lat, 2)) %>%
  group_by(long_round, lat_round) %>%
  summarise(total_count = n()) %>%
  ungroup() 
#
# binding the hash_homeworkout_locations_grp and hash_gym_locations_grp together 
# so we could plot them easily in the same plot
#
hash_homeworkout_locations_grp_m <- hash_homeworkout_locations_grp %>%
  mutate(tweet = '#homeworkout')
#
hash_gym_locations_grp_m <- hash_gym_locations_grp %>%
  mutate(tweet = '#gym')
#
bind_location_grp <- rbind(hash_homeworkout_locations_grp_m, hash_gym_locations_grp_m)

#
# Plots tweet data about #homeworkout and #gym grouping close tweets and 
# using larger points to show higher frequency
#
# we first have to create a basemap of the globe
# the theme_map() function cleans up the look of the map
#
world_basemap <- ggplot() +
  borders("world", colour = "gray85", fill = "gray80") +
  theme_map()
# 
# worldmap visual
#
world_basemap + 
  geom_point(data = bind_location_grp,
             aes(long_round, lat_round, size = total_count, colour = tweet),
              alpha = 0.4) + 
  coord_fixed() +
  labs(title = "Twitter Activity and locations of\n#homeworkout vs #gym",
       size = "Number of Tweets",
       colour = 'Tweet',
       caption = "Source: Data collected from Twitter's REST API via rtweet") +
  theme(
    plot.title = element_text(size=18, face = 'bold'),
    plot.caption = element_text(size=11, face = 'italic'),
    legend.background = element_rect(colour = "black"),
    legend.title = element_text(size = 11),
    legend.text = element_text(size = 10),
    legend.key.size = unit(0.8, "lines")  
  ) +
  scale_color_manual(values = c("#homeworkout" = "tomato", 
                               "#gym" = "steelblue"))
```



```{r data_clean_up, include=FALSE}
#
# remove urls
hash_homeworkout_tweets$stripped_text <- gsub("http.*","",  hash_homeworkout_tweets$text)
hash_homeworkout_tweets$stripped_text <- gsub("https.*","", hash_homeworkout_tweets$stripped_text)
hash_homeworkout_tweets$stripped_text <- gsub("amp","", hash_homeworkout_tweets$stripped_text)
#
hash_gym_tweets$stripped_text <- gsub("http.*","",  hash_gym_tweets$text)
hash_gym_tweets$stripped_text <- gsub("https.*","", hash_gym_tweets$stripped_text)
hash_gym_tweets$stripped_text <- gsub("amp","", hash_gym_tweets$stripped_text)
#
# remove punctuation, convert to lowercase, add id for each tweet:
hash_homeworkout_tweets_clean <- hash_homeworkout_tweets %>%
  select(stripped_text) %>% 
  mutate(tweetnumber = row_number()) %>% # creates a new variable denoting the tweet number
  unnest_tokens(word, stripped_text)
#
hash_gym_tweets_clean <- hash_gym_tweets %>%
  select(stripped_text) %>% 
  mutate(tweetnumber = row_number()) %>% # creates a new variable denoting the tweet number
  unnest_tokens(word, stripped_text)
#
## clean stop words
data("stop_words")
#
my_stop_words <- data.frame(word = c("day",'gym',
                                     'homeworkout', 'home', 
                                     '7','30', # must refer to workout programs who last 7 or 30 days, 
                                     'workout',
                                     'calculus', 'assignments' # calculus and assignments apppear a lot in #homeworkout, we assume that they are related to homework
                                    )
                            ) 
#
hash_homeworkout_tweets_clean  <- hash_homeworkout_tweets_clean %>%
  anti_join(stop_words)
#
hash_homeworkout_tweets_clean  <- hash_homeworkout_tweets_clean %>%
  anti_join(my_stop_words)
#
#
hash_gym_tweets_clean  <- hash_gym_tweets_clean %>%
  anti_join(stop_words)
#
hash_gym_tweets_clean  <- hash_gym_tweets_clean %>%
  anti_join(my_stop_words)
#

```
```{r unique_words_pre_code, include=FALSE}
# in order to have a common graph for both, we need to bind them together in the same df,
# first we create a new column with mutate to state their tweet,
# then we bind them using rbind()
#


hash_homeworkout_tweets_clean_m <- hash_homeworkout_tweets_clean %>% 
  mutate(tweet = "#homeworkout")
#
hash_gym_tweets_clean_m <- hash_gym_tweets_clean %>% 
  mutate(tweet = "#gym")
#
binded_clean <- rbind(hash_homeworkout_tweets_clean_m, hash_gym_tweets_clean_m)
```

```{r,  eval = FALSE, include = FALSE, message=FALSE}
# we did not used this because we had already many plots
# common words
# converting the striped text columns to a dataframe so we can use the rbind()
# then the colnames() is used to set the same column name in order to bind the together
# 
df_stripped_text_home <- as.data.frame(hash_homeworkout_tweets$stripped_text)
colnames(df_stripped_text_home) <- 'text'
#
df_stripped_text_gym <- as.data.frame(hash_gym_tweets$stripped_text)
colnames(df_stripped_text_gym) <- 'text'
# binding them together
common_words_together <- rbind(df_stripped_text_home, df_stripped_text_gym)
#
# cleaning
common_words_together_clean <- common_words_together %>%
  select(text) %>% 
  mutate(tweetnumber = row_number()) %>% # create new variable denoting the tweet number
  unnest_tokens(word, text)
#
common_words_together_clean <- common_words_together_clean %>%
  anti_join(stop_words)
#
common_words_together_clean <- common_words_together_clean %>%
  anti_join(my_stop_words)
#
#
# plotting
common_words_together_clean %>%
  count(word, sort = TRUE) %>% # count of number of occurrences of each word and sort according to count
  head(10) %>% # extract top 10 words
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(fill = "pink", color = "red") +
  coord_flip() +
  labs(x = "Unique Words",
       y = "Frequency",
       caption = "Source: Data collected from Twitter's REST API via rtweet",
       title = "Count of unique common words") + 
  theme(axis.text = element_text(size = 16, color = "black"), 
        axis.title = element_text(size = 16, color = "black"),
        title = element_text(size = 18),
        plot.caption = element_text(size = 11, face = "italic"))

```
```{r world_clouds_pre, include=FALSE}
#
# we calculate the appearance of each word and calculate their frequency
#
hash_homeworkout_tweets_clean_2 <- hash_homeworkout_tweets_clean %>%
  count(word, sort = TRUE) %>% 
  mutate(freq = n / sum(n))
#
hash_gym_tweets_clean_2 <- hash_gym_tweets_clean %>%
  count(word, sort = TRUE) %>% 
  mutate(freq = n / sum(n))
#
```
```{r world_cloud_#homeworkout, eval=FALSE, include=FALSE}
# we did not used the worldclouds since we have the bar-charts which are more illustrative
wordcloud2(hash_homeworkout_tweets_clean_2, size = 1.5, shape = 'star', color = 'random-dark')
```
```{r world_cloud_#gym, eval =FALSE, include=FALSE}
wordcloud2(hash_gym_tweets_clean_2, size = 1.5)
```

## Sentiment Analysis

```{r unique_words_graph, echo=FALSE}
binded_clean %>%
  group_by(tweet) %>%
  count(word) %>%
  arrange(desc(n)) %>%
  slice_head(n = 10) %>%
  ggplot(aes(x = word, y = n, fill = tweet)) +
  geom_col() +
  coord_flip() +
  labs(x = "Unique Words",
       y = "Frequency",
       fill = 'Tweet',
       caption = "Source: Data collected from Twitter's REST API via rtweet",
       title = "Count of unique words found in tweets with\n#homeworkout and #gym") + 
  theme(legend.position = 'none',
        axis.text = element_text(size = 13, color = "black"),
        axis.title = element_text(size = 14, color = "black"),
        plot.title = element_text(size = 15, hjust = 0, face = 'bold'),
        plot.caption = element_text(size = 11, face = "italic")) +
  scale_fill_manual(values = c("#homeworkout" = "tomato", 
                               "#gym" = "steelblue")) +
  facet_grid(tweet~., scales ='free') 
```

## Sentiment Analysis
```{r top_positive_negative_words_prep, include=FALSE}
#
bing_home_word_counts <- hash_homeworkout_tweets_clean %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  mutate(word = reorder(word, n))
#
bing_gym_word_counts <- hash_gym_tweets_clean %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  mutate(word = reorder(word, n))
#
# in order to plot them together, they should be in the same dataframe
#
bing_home_word_counts_m <-bing_home_word_counts %>%
                          mutate(hash = '#homeworkout')
#
bing_gym_word_counts_m <- bing_gym_word_counts %>%
                            mutate(hash = '#gym')
#
# rbind() together
#
bing_together <- rbind(bing_home_word_counts_m, bing_gym_word_counts_m)
```

```{r plot_top_positive_negative_words, echo=FALSE}
#
# # Since "these words" are considered for its literal meaning, we include it in the stopwords
#
# bing_together <- data.frame(word = c("funny"))
# plot top words
#
bing_together %>%
  group_by(sentiment, hash) %>%
  arrange(desc(n)) %>%
  slice_head(n = 5) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(title = "Most common Positive and Negative words\nin tweets on #homeworkout vs #gym",
       caption = "Source: Data collected from Twitter's REST API via rtweet",
       y = "Sentiment score",
       x = NULL ) +
  theme(axis.text = element_text(size = 16, color = "black"),
        axis.title = element_text(size = 16, color = "black"),
        plot.title = element_text(size = 18, hjust = 0, face = 'bold'),
        plot.caption = element_text(size = 11, face = "italic"),
        legend.title = element_text(size = 16, face = 'bold')) +
  scale_fill_manual(values = c("positive"="steelblue",
                              "negative"="tomato"))+
  facet_wrap(hash ~ sentiment, scales = 'free_y')

```
```{r plot_home, eval=FALSE, include=FALSE}
#
# we did not included those graphs in the final presentation since we have the grid graph illustrating both
# there are included here for your reference
plot_bing_words(bing_home_word_counts, "Most common Positive and Negative words\nin tweets on #homeworkout")
```
```{r plot_gym, eval=FALSE, include=FALSE}
#
# we did not included those graphs in the final presentation since we have the grid graph illustrating both
# there are included here for your reference
plot_bing_words(bing_gym_word_counts, "Most common Positive and Negative words\nin tweets on #gym")
```


## Sentiment Analysis
```{r calculating_sentiment_scores, echo=FALSE, message=FALSE}
#
# using the sentiment_score function we made to create a df with the sentiment score
#
homeworkout_sentiment <- sentiment_score(hash_homeworkout_tweets_clean)
gym_sentiment <- sentiment_score(hash_gym_tweets_clean)
# Add a variable to indicate the tweet
#
homeworkout_sentiment <- homeworkout_sentiment %>% 
  mutate(tweet = "#homeworkout")
#
gym_sentiment <- gym_sentiment %>% 
  mutate(tweet = "#gym")
#
# using the sentiment_stats function we made to calculate the mean and median 
# sentiment scores of #homeworkout and #gym
homeworkout_sentiment_mean <- sentiment_stats(homeworkout_sentiment)[1]
homeworkout_sentiment_median <-  sentiment_stats(homeworkout_sentiment)[2]
gym_sentiment_mean <- sentiment_stats(gym_sentiment)[1]
gym_sentiment_median <- sentiment_stats(gym_sentiment)[2]
#
# Work out the means for each topic
# so that these can be added to the graph for each topic
# as a line and as a numerical value
#
sentiments_bind <- rbind(homeworkout_sentiment, gym_sentiment)
#
sentiment_stats_both <- sentiments_bind %>% 
  group_by(tweet) %>% 
  summarize(mean_score = mean(score),
            median_score = median(score)) 
#
# Perform the plot
#
ggplot(sentiments_bind, 
       aes(x = score, # Sentiment score on x-axis
           fill = tweet)) + # Fill bars with a color according to the topic
  geom_bar() + # geom_bar will do the tabulation for you :-)
  geom_vline(aes(xintercept = mean_score), 
             data = sentiment_stats_both) +
  # Add a vertical line at the mean scores, calculated and stored in sentiment_mean_both above
  geom_text(aes(x = mean_score, 
                y = Inf, 
                label = paste('m=', signif(mean_score, 3),sep = '')), #decimal points
            vjust = 3,
            hjust = 1,
            data = sentiment_stats_both) +
  scale_x_continuous(breaks = -15:15, 
                     minor_breaks = NULL) + # Show integers; set this to a suitably large range
  scale_fill_manual(values = c("#homeworkout" = "tomato", 
                               "#gym" = "steelblue")) + # Specify your own colours
  labs(x = "Sentiment Score" , 
       y = "Number of tweets", 
       fill = "Tweet",
       title = "Distribution of sentiment scores for #homeworkout and #gym",
       caption = "Source: Data collected from Twitter's REST API via rtweet") +
  facet_grid(tweet ~ .) +
  theme(legend.position = "none",  
        axis.text = element_text(size = 13, color = "black"),
        axis.title = element_text(size = 14, color = "black"),
        plot.title = element_text(size = 15, hjust = 0, face = 'bold'),
        plot.caption = element_text(size = 11, face = "italic"))
```

## Summary Statistics
```{r summary_stats_of_sentient_score, echo=FALSE, message=FALSE, comment=' '}
#
library(knitr)
# summary statistics
#
sentiments_stats_table <- sentiments_bind %>%
        group_by(tweet) %>%
        sentiment_stats()

#we use the kable() function to format our table, aling sets the columns content into center for eatch column ("c"),
kable(sentiments_stats_table,
     digits = 2, # we limit the amount of decimal digits to 2
     align = 'ccccc', # we algn each columns content to the center - "c" = center
     col.names = c("Tweet", "Mean", "Median", "Variance", "Sd", "IQR",'Min','Max')) # we set our column names

```


## Statistical Testing

__T-test assumptions:__

1. Scale of measurement
2. Simple random sampling
    -   _not everyone uses social media the same way_
    -   _language restriction_
3. Adequacy of sample.
4. Equality of variance in standard deviation. 
    -   _one point difference in variance_
5. Normality of data distribution



## Statistical Testing
```{r density_plots, echo=FALSE}
# plotting density plots to have a better visualization of our distributions
# t-test follows some assumptions:
# 1. scale of measurement,   # yes
# 2. simple random sampling, # not all users are the same expressful (some of them do not post their opinion), 
# and also not all the gym/homeworkout users use twitter, also we included only english tweets
# 3. normality of data distribution, # check with the shapiro.test and histogramms (gym might be but homeworkout appear bimodal)
# 4. adequacy of sample size # yes more than 1000 tweets
# 5. equality of variance in standard deviation. # Variance of #homewkorkout is 2.02 while for gym is 3.11
h_mean <- as.numeric(homeworkout_sentiment_mean)
h_median <- as.numeric(homeworkout_sentiment_median)
g_mean <- as.numeric(gym_sentiment_mean)
g_median <- as.numeric(gym_sentiment_median)

p <- ggplot(sentiments_bind, aes(x=score, color= tweet, fill = tweet))+
  geom_density(alpha = 0.2) +
  labs(title = "Density plots of sentiment scores of #homeworkout vs #gym",
       caption = "Source: Data collected from Twitter's REST API via rtweet")+
  
  scale_fill_manual(values = c("#homeworkout" = "tomato", 
                               "#gym" = "steelblue")) +
  scale_colour_manual(values = c("#homeworkout" = "tomato", 
                               "#gym" = "steelblue")) +
   scale_x_continuous(breaks = -15:15, 
                     minor_breaks = NULL,
                     limits = c(-5,8))+
  theme(legend.position = 'bottom',
        plot.title = element_text(size=13, face='bold'),
        plot.caption = element_text(size=11, face='bold', vjust = 0))
  

p + geom_vline(aes(xintercept=h_mean),
               color="tomato",linetype="dashed", size=0.6) +
    geom_text(aes(x = h_mean),
                y = 0.2,
                label = paste('Mh=',signif(h_mean, 3), sep = ''), color="tomato")+
    geom_vline(aes(xintercept=g_mean),
               color="steelblue",linetype="twodash", size=0.6) +
    geom_text(aes(x = g_mean),
                y = 0.1,
                label = paste('Mg=',signif(g_mean, 3), sep = ''), color="steelblue")
```
```{r visual_inspection_box_plot, echo=FALSE, eval=FALSE, include=FALSE}
# we decided not to include the boxplot since we included the density plots, and due to time constraints as well
ggplot(sentiments_bind, aes(x = tweet, y = score, fill = tweet)) + 
  geom_boxplot(varwidth = TRUE) +
  labs(title = "Box plots of sentiment scores",
       x = NULL,
       y = "Sentiment Score",
       fill = 'Tweet',
       caption = "Source: Data collected from Twitter's REST API via rtweet" )+
  scale_fill_manual(values = c("#homeworkout" = "tomato",
                               "#gym" = "steelblue")) +
  theme(legend.text = element_text(size = 11),
        axis.text = element_text(size = 16, color = "black"),
        axis.title = element_text(size = 16, color = "black"),
        plot.title = element_text(size = 17, hjust = 0, face = 'bold'),
        plot.caption = element_text(size = 11, face = "italic"),
        legend.title = element_text(size = 16, face = 'bold'))
```


## Statistical Testing
### Shapiro–Wilk test
```{r test_shapiro, echo=FALSE, comment=' '}
# shapirto-wilk's method
# null hypothesis of these tests is that "sample distribution is normal". 
# If the test is significant, the distribution is non-normal.

# H0 = distribution is normal
# H1 = distribution is not normal
normal_test_home_p <- shapiro.test(homeworkout_sentiment$score)$p.value
# p-value < 0.05, the distribution is not normal
normal_test_gym_p <- shapiro.test(gym_sentiment$score)$p.value
# p-value < 0.05, the distribution is not normal
```
 - $H_0:$ _The distribution of our data sample is not significant different, from a normal distribution_

 - $H_1:$ _The distribution of our data sample is significant different, from a normal distribution_
  
__#homeworkout__ : $P-value= 2.8 \times 10^{-16} < 0.05$\
__#gym__         : $P-value=    1 \times 10^{-9} < 0.05$\
At the 5% significance level, we reject the null hypothesis $H_0$.\ 

__There is a significance difference between the distributions of the samples and the normal distribution__
  

## Statistical Testing
### Mann-Whitney U test
```{r test_Mann_Whitney_U, echo=FALSE}
# Since the distributions are not normal we should use the  Mann-Whitney-Wilcoxon test.
# H0, the distributions of both populations are equal
# H1, is that the mean ranks are not equal.
w_test <- wilcox.test(score ~ tweet, data=sentiments_bind)$p.value
# p-value < 0.05 
#The p-value = 3.6 x 10^-6 < 0.5. Therefore, at the 5% significance level, we reject the null hypothesis, and we #conclude that the sentiment scores between tweets with #gym and #homeworkout are significant different.
#
```
 - $H_0:$ _The The mean ranks of the two groups are equal_

 - $H_1:$ _The mean ranks of the two groups are not equal_

$P-value= 3.6 \times 10^{-16} < 0.05$\
At the 5% significance level, we reject the null hypothesis $H_0$.\

__The sentiment mean rank scores between tweets with #gym and #homeworkout are significant different__


## Conclusions

 - Observe a new trend in fitness industry where people prefer home workouts.

 - Businesses can use this information to produce new products and services therefore increase their profits in a competitive industry.


## References
 - Faraway, J.J, (2004). _Linear Models with R_, Chapman and Hall/CRC
 - Witte, R.S. and Witte J.S, (2016). _Statistics_, 11th ed. Willey
 - Kales, S., (2019). The rise of digital fitness: can the new wave of high-intensity home workouts replace the gym?. _The Guardian_ [online]. 19 August. Updated 19 August 2019, 10:00. [Viewed 08 December 2020]. Available from: https://www.theguardian.com/lifeandstyle/2019/aug/19/the-rise-of-digital-fitness-can-the-new-wave-of-high-intensity-home-workouts-replace-the-gym

## References

  - Strelnikova, O.,  _Cartoon doing lunges in home_ [Online]. [Viewed 09 December 2020]. Available from: https://www.dreamstime.com/stay-home-doing-exercise-online-keep-fit-positive-man-laptop-training-gym-sport-internet-fitness-workout-healthy-image178965240?hcb=1
 - Macrovector. _Cartoon doing barbells in gym_ [Online]. [Viewed 09 December 2020]. Available at: https://www.freepik.com/free-vector/man-training-arm-muscles-with-dumbbells-gym-cartoon_3795972.htm
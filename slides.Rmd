---
title: |-
  MATH513
  Big Data and Social Network Visualization
  Practical #Homeworkout vs #Gym
author: '"10576468, ,10546918, Sid ID'
date: "`r Sys.Date()`"
output:
  ioslides_presentation: default
  beamer_presentation: default
  logo: logo.jpg
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r twitter_request, eval = FALSE, include = FALSE}
  # making request to the tweeter API
my_token <- create_token(
  app = "KTweeter_analysis_app",
  consumer_key = "gcwB0SC0FfKAz6YfyTIw3wUGd",
  consumer_secret = "0HeRgRxBIfVJVxSmjjaJdFZtIs9nMM1lVpVHv2c3mtnOtp0qoG",
  access_token = "1315266158052990977-uWAIGpx2rlbGPyuICYwmGrj1UtO7Vc",
  access_secret = "7JVOj6aVqvWy2lC6ehN7VHHrmB5RwFsr0Xopmtd6ABRQO")
###########

# we filtered our search to receive tweets only with English language,  
# also we included r-tweets
hash_homeworkout_tweets <- search_tweets(q = "#homeworkout",
                        n = 1000, lang = "en", include_rts = TRUE)

hash_gym_tweets <- search_tweets(q = "#gym",
                                 n = 1000, lang = "en", include_rts = TRUE)
```

```{r, echo=FALSE, include=FALSE}
# loading libraries
library(rtweet)
library(ggplot2)
library(dplyr)
library(ggthemes)
library(readr)
library(jsonlite)
library(tidytext)
library(wordcloud)
library(wordcloud2)
library(tidyr)
library(maps)
library(scales)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#
#setting the working directory
setwd("C:/Users/kwsta/master_projects/Math513/CRW_presentation")
#
# makes a dataframe out of a json file
# read the data from the .json files
hash_homeworkout_tweets <- stream_in(file("hash_homeworkout_tweets.json"))
hash_gym_tweets <- stream_in(file("hash_gym_tweets.json"))
#
```

```{r function_chunk, include=FALSE}
#
#creating functions
#
# Top locations
# 
plot_top_locations <- function(x, title_end = ' ', fill = 'blue') {x %>%
    count(location_rec, sort = TRUE) %>%
    mutate(location_rec = reorder(location_rec, n)) %>%
    na.omit() %>%
    head(10) %>%
    ggplot(aes(x = location_rec, y = n))+
    geom_col(fill = fill ) +
    coord_flip() +
    labs(x = "Location",
         y = "Number of tweets",
         title = paste("Top locations which tweeted:", title_end, sep = ' '),
         caption = 'Data Source: Twitter (derived using rtweet)') + 
    theme(axis.text = element_text(size = 16, color = "black"), 
          axis.title = element_text(size = 16, color = "black"),
          plot.title = element_text(size = 18, face = 'bold'),
          plot.caption = element_text(size = 11, face = 'italic'))
    }
#  
# Joining similar locations
#
join_similar_locations <- function(x){x %>%
    mutate(location_rec = 
             recode(location, 'United States' = 'USA', 'United State' = 'USA',
                    'US' = 'USA', 'Chicago' = 'Chicago, IL', 
                    "London, England" = "London",
                    "London,UK" = "London",
                    "London, UK" = "London", 
                    "South East, England" = 'United Kingdom',
                    "UK" = "United Kingdom", "u.k." = "United Kingdom",
                    "United Kingdom, EU" = "United Kingdom",
                    "England, United Kingdom" = "United Kingdom",
                    "united kingdom" = "United Kingdom",
                    "EU" = "European Union"
                    )
           )
  
    }
#
# function for the creation of a data frame with just the tweet texts, usernames and location data
#
create_lean_df <- function(df){ data.frame(date_time = df$created_at,
                                            username = df$screen_name,
                                            tweet_text = df$text,
                                            long = df$lng,
                                            lat = df$lat)
}
#
# function for ploting top negative and positive words
#
plot_bing_words <- function(df, title = '') {df %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(title = title,
       caption = 'Data Source: Twitter (derived using rtweet)',
       y = "Sentiment",
       x = NULL) +
  theme(axis.text = element_text(size = 14, color = "black"), 
        axis.title = element_text(size = 14, color = "black"),
        plot.title = element_text(size = 18, hjust = 0, face = 'bold'),
        plot.caption = element_text(size = 11, face = "italic"))
}
#
# function to create a df with the score of sentiments sentiments
#
sentiment_score <- function(x){ x %>%
    inner_join(get_sentiments("bing")) %>%
    count(tweetnumber, sentiment) %>%
    spread(sentiment, n, fill = 0) %>% # negative and positive sentiment in separate columns
    mutate(score = positive - negative) # the score is the sum of positive and negative appearances of each word in a tweet's text
}
#
# function for calculating the sentiment score means
#
sentiment_stats <- function(x){x %>%
    summarise(mean_score = mean(score),
              median_score = median(score),
              variance_score = var(score),
              sd_score = sd(score),
              IQR_score = IQR(score)
              )
}
```

```{r, echo=FALSE}
#
# Cleaning data
#
# dealing with blank locations
#
hash_homeworkout_tweets$location[hash_homeworkout_tweets$location==""] <- NA
hash_gym_tweets$location[hash_gym_tweets$location==""] <- NA
#
# A location named "Nothing just happens." spotted. We clean it up since is not meaningful
#
hash_gym_tweets$location[hash_gym_tweets$location=="Nothing just happens."] <- NA
#
# Joining the similar locations with the join_similar_locations function we made previously
#
hash_homeworkout_tweets_recoded <- join_similar_locations(hash_homeworkout_tweets)
hash_gym_tweets_recoded <- join_similar_locations(hash_gym_tweets)
```
## TOP LOCATIONS
```{r top_location_home, echo=FALSE, eval=FALSE}
# 
# Visuals for #homeworkout
# we did not included those graphs in the final presentation since we have the grid graph illustrating both
# there are included here for your reference and check that facet_grid graph work properly
plot_top_locations(hash_homeworkout_tweets_recoded, '#homeworkout', "tomato")
```

```{r top_location_gym, echo=FALSE, eval=FALSE}
#
# visuals for #gym
# we did not included those graphs in the final presentation since we have the grid graph illustrating both
plot_top_locations(hash_gym_tweets_recoded, '#gym', "steelblue")
length(hash_homeworkout_tweets$location[hash_homeworkout_tweets$location==NA])
length(hash_gym_tweets$location[hash_gym_tweets$location==NA])
```

```{r top_locations_facet_grid, echo=FALSE, message=FALSE}
#
# we bind the "hash_homeworkout_tweets" and "hash_gym_tweets" in order to create a facet_grid
#
# firstly we use mutate in order to create a column called tweet so we can 
# group_by them using their particular value (#homeworktout and #gym)
#
hash_homeworkout_tweets_m <- hash_homeworkout_tweets_recoded %>%
  mutate(tweet = '#homeworkout')
#
hash_gym_tweets_m <- hash_gym_tweets_recoded %>%
  mutate(tweet = '#gym')
#
# use rbind() to bind the together
#
binded_tweets <- rbind(hash_homeworkout_tweets_m, hash_gym_tweets_m)
#
# use the join_similar_locations function we made
#
binded_tweets_r <- join_similar_locations(binded_tweets)
#
# visual
#
binded_tweets_r %>%
  select(tweet, location_rec) %>% # just selecting the columns we need
  group_by(tweet) %>%
  count(location_rec) %>%
  na.omit() %>%
  arrange(desc(n)) %>%  # we faced a problem using head() with the binded data, 
  slice_head(n = 5) %>% # so instead we used arrange(desc) to sort the data, and then slice_head(5) to pick the first 5 results
  ggplot(aes(x = location_rec,y = n, fill = tweet)) +
  geom_col() +
  coord_flip() +
  labs(x = "Top Locations",
       y = "Number of tweets",
       fill = "Tweets",
       title = "Top locations of #gym and #homeworkout tweets",
       caption = 'Data Source: Twitter (derived using rtweet)') +
  theme(axis.text = element_text(size = 16, color = "black"),
        axis.title = element_text(size = 16, color = "black"),
        plot.title = element_text(size = 18, face = 'bold'),
        legend.title = element_text(size = 16, face = 'bold'),
        plot.caption = element_text(size = 11, face = 'italic')) +
  scale_fill_manual(values = c("#homeworkout" = "tomato", 
                               "#gym" = "steelblue")) +
  facet_grid(tweet~ ., scales ='free')
```

## WORLD MAP
```{r worldmap_pre_code, echo = FALSE, message=FALSE}
# 
# create variables indicating latitude and longitude using all available 
# tweet and profile geo-location data
#
hash_homeworkout_tweets_map <- lat_lng(hash_homeworkout_tweets)
hash_gym_tweets_map <- lat_lng(hash_gym_tweets)
#
# we create new data frame with just the tweet texts, user-names and location data
# using the create_lean_df function we made (see "function_chunk" chunk in the beginning of the script)
#
hash_homeworkout_tweets_map_s <- create_lean_df(hash_homeworkout_tweets_map)
#
hash_gym_tweets_map_s <- create_lean_df(hash_gym_tweets_map)
#
# remove na values
#
hash_homeworkout_locations  <- hash_homeworkout_tweets_map_s %>%
  na.omit()
#
hash_gym_locations  <- hash_gym_tweets_map_s %>%
  na.omit()
#
# round latitude and longitude and group close tweets
#
hash_homeworkout_locations_grp <- hash_homeworkout_locations %>%
  mutate(long_round = round(long, 2),
         lat_round = round(lat, 2)) %>%
  group_by(long_round, lat_round) %>%
  summarise(total_count = n()) %>%
  ungroup() 
#
hash_gym_locations_grp <- hash_gym_locations %>%
  mutate(long_round = round(long, 2),
         lat_round = round(lat, 2)) %>%
  group_by(long_round, lat_round) %>%
  summarise(total_count = n()) %>%
  ungroup() 
#
# binding the hash_homeworkout_locations_grp and hash_gym_locations_grp together 
# so we could plot them easily in the same plot
#
hash_homeworkout_locations_grp_m <- hash_homeworkout_locations_grp %>%
  mutate(tweet = '#homeworkout')
#
hash_gym_locations_grp_m <- hash_gym_locations_grp %>%
  mutate(tweet = '#gym')
#
bind_location_grp <- rbind(hash_homeworkout_locations_grp_m, hash_gym_locations_grp_m)

#
# Plots tweet data about #homeworkout and #gym grouping close tweets and 
# using larger points to show higher frequency
#
# we first have to create a basemap of the globe
# the theme_map() function cleans up the look of the map
#
world_basemap <- ggplot() +
  borders("world", colour = "gray85", fill = "gray80") +
  theme_map()
# 
# worldmap visual
#
world_basemap + 
  geom_point(data = bind_location_grp,
             aes(long_round, lat_round, size = total_count, colour = tweet),
              alpha = 0.4) + 
  coord_fixed() +
  labs(title = "Twitter Activity and locations of #homeworkout vs #gym",
       size = "Number of Tweets",
       colour = 'Tweet',
       caption = 'Data Source: Twitter (derived using rtweet)') +
  theme(
    plot.title = element_text(size=22, face = 'bold'),
    plot.caption = element_text(size=11, face = 'italic'),
    legend.background = element_rect(colour = "black"),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 13),
    legend.key.size = unit(1, "lines")  
  ) +
  scale_color_manual(values = c("#homeworkout" = "tomato", 
                               "#gym" = "steelblue"))
```

## SENTIMENT ANALYSIS
```{r data_clean_up, include=FALSE}
#
# remove urls
hash_homeworkout_tweets$stripped_text <- gsub("http.*","",  hash_homeworkout_tweets$text)
hash_homeworkout_tweets$stripped_text <- gsub("https.*","", hash_homeworkout_tweets$stripped_text)
hash_homeworkout_tweets$stripped_text <- gsub("amp","", hash_homeworkout_tweets$stripped_text)
#
hash_gym_tweets$stripped_text <- gsub("http.*","",  hash_gym_tweets$text)
hash_gym_tweets$stripped_text <- gsub("https.*","", hash_gym_tweets$stripped_text)
hash_gym_tweets$stripped_text <- gsub("amp","", hash_gym_tweets$stripped_text)
#
# remove punctuation, convert to lowercase, add id for each tweet:
hash_homeworkout_tweets_clean <- hash_homeworkout_tweets %>%
  select(stripped_text) %>% 
  mutate(tweetnumber = row_number()) %>% # creates a new variable denoting the tweet number
  unnest_tokens(word, stripped_text)
#
hash_gym_tweets_clean <- hash_gym_tweets %>%
  select(stripped_text) %>% 
  mutate(tweetnumber = row_number()) %>% # creates a new variable denoting the tweet number
  unnest_tokens(word, stripped_text)
#
## clean stop words
data("stop_words")
#
my_stop_words <- data.frame(word = c("day",'gym',
                                     'homeworkout', 'home', 
                                     '7','30', # must refer to workout programs who last 7 or 30 days, 
                                     'workout',
                                     'calculus', 'assignments' # calculus and assignments apppear a lot in #homeworkout, we assume that they are related to homework
                                    )
                            ) 
#
hash_homeworkout_tweets_clean  <- hash_homeworkout_tweets_clean %>%
  anti_join(stop_words)
#
hash_homeworkout_tweets_clean  <- hash_homeworkout_tweets_clean %>%
  anti_join(my_stop_words)
#
#
hash_gym_tweets_clean  <- hash_gym_tweets_clean %>%
  anti_join(stop_words)
#
hash_gym_tweets_clean  <- hash_gym_tweets_clean %>%
  anti_join(my_stop_words)
#

```

```{r unique_words_pre_code, include=FALSE}
# in order to have a common graph for both, we need to bind them together in the same df,
# first we create a new column with mutate to state their tweet,
# then we bind them using rbind()
#


hash_homeworkout_tweets_clean_m <- hash_homeworkout_tweets_clean %>% 
  mutate(tweet = "#homeworkout")
#
hash_gym_tweets_clean_m <- hash_gym_tweets_clean %>% 
  mutate(tweet = "#gym")
#
binded_clean <- rbind(hash_homeworkout_tweets_clean_m, hash_gym_tweets_clean_m)
```

```{r unique_words_graph, echo=FALSE}
binded_clean %>%
  group_by(tweet) %>%
  count(word) %>%
  arrange(desc(n)) %>%
  slice_head(n = 10) %>%
  ggplot(aes(x = word, y = n, fill = tweet)) +
  geom_col() +
  coord_flip() +
  labs(x = "Unique Words",
       y = "Frequency",
       fill = 'Tweet',
       caption = 'Data Source: Twitter (derived using rtweet)',
       title = "Count of unique words found in tweets with\n#homeworkout and #gym") + 
  theme(axis.text = element_text(size = 16, color = "black"),
        axis.title = element_text(size = 16, color = "black"),
        plot.title = element_text(size = 18, hjust = 0, face = 'bold'),
        plot.caption = element_text(size = 11, face = "italic"),
        legend.title = element_text(size = 16, face = 'bold')) +
  scale_fill_manual(values = c("#homeworkout" = "tomato", 
                               "#gym" = "steelblue")) +
  facet_grid(tweet~., scales ='free') 
binded_clean %>% group_by(tweet) %>%count(word) %>% arrange(desc(n))
```

```{r,  eval = FALSE, include = FALSE}}
# we did not used this because we had already many plots
#common words
# converting the striped text columns to a dataframe so we can use the rbind()
# then the colnames() is used to set the same column name in order to bind the together
# 
df_stripped_text_home <- as.data.frame(hash_homeworkout_tweets$stripped_text)
colnames(df_stripped_text_home) <- 'text'
#
df_stripped_text_gym <- as.data.frame(hash_gym_tweets$stripped_text)
colnames(df_stripped_text_gym) <- 'text'
# binding them together
common_words_together <- rbind(df_stripped_text_home, df_stripped_text_gym)
#
# cleaning
common_words_together_clean <- common_words_together %>%
  select(text) %>% 
  mutate(tweetnumber = row_number()) %>% # create new variable denoting the tweet number
  unnest_tokens(word, text)
#
common_words_together_clean <- common_words_together_clean %>%
  anti_join(stop_words)
#
common_words_together_clean <- common_words_together_clean %>%
  anti_join(my_stop_words)
#
#
# plotting
common_words_together_clean %>%
  count(word, sort = TRUE) %>% # count of number of occurrences of each word and sort according to count
  head(10) %>% # extract top 10 words
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(fill = "pink", color = "red") +
  coord_flip() +
  labs(x = "Unique Words",
       y = "Frequency",
       caption = 'Data Source: Twitter (derived using rtweet)',
       title = "Count of unique common words") + 
  theme(axis.text = element_text(size = 16, color = "black"), 
        axis.title = element_text(size = 16, color = "black"),
        title = element_text(size = 18),
        plot.caption = element_text(size = 11, face = "italic"))

```

## world cloud homeworkout

```{r world_clouds_pre, include=FALSE}
#
# we calculate the appearance of each word and calculate their frequency
#
hash_homeworkout_tweets_clean_2 <- hash_homeworkout_tweets_clean %>%
  count(word, sort = TRUE) %>% 
  mutate(freq = n / sum(n))
#
hash_gym_tweets_clean_2 <- hash_gym_tweets_clean %>%
  count(word, sort = TRUE) %>% 
  mutate(freq = n / sum(n))
#
```

```{r world_cloud_#homeworkout, echo = FALSE}
wordcloud2(hash_homeworkout_tweets_clean_2, size = 1.5, shape = 'star', color = 'random-dark')
```

## World cloud gym

```{r world_cloud_#gym, echo = FALSE}
wordcloud2(hash_gym_tweets_clean_2, size = 1.5)
```

## Top negative and positive words

```{r top_positive_negative_words_prep, include=FALSE}
#
bing_home_word_counts <- hash_homeworkout_tweets_clean %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  mutate(word = reorder(word, n))
#
bing_gym_word_counts <- hash_gym_tweets_clean %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  mutate(word = reorder(word, n))
#
# in order to plot them together, they should be in the same dataframe
#
bing_home_word_counts_m <-bing_home_word_counts %>%
                          mutate(hash = '#homeworkout')
#
bing_gym_word_counts_m <- bing_gym_word_counts %>%
                            mutate(hash = '#gym')
#
# rbind() together
#
bing_together <- rbind(bing_home_word_counts_m, bing_gym_word_counts_m)
```

```{r plot_top_positive_negative_words, echo=FALSE}
#
# # Since "these words" are considered for its literal meaning, we include it in the stopwords
#
# bing_together <- data.frame(word = c("funny"))
# plot top words
#
bing_together %>%
  group_by(sentiment, hash) %>%
  arrange(desc(n)) %>%
  slice_head(n = 5) %>%
  ggplot(aes(word, n, fill = sentiment, color = hash)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(title = "Most common Positive and Negative words\nin tweets on #homeworkout vs #gym",
       caption = 'Data Source: Twitter (derived using rtweet)',
       y = "Sentiment score",
       x = NULL ) +
  theme(axis.text = element_text(size = 16, color = "black"),
        axis.title = element_text(size = 16, color = "black"),
        plot.title = element_text(size = 18, hjust = 0, face = 'bold'),
        plot.caption = element_text(size = 11, face = "italic"),
        legend.title = element_text(size = 16, face = 'bold')) +
  facet_wrap(hash ~ sentiment, scales = 'free_y') +
  scale_fill_manual(values = c("positive" = "blue", 
                             "negative" = "red",
                             "#gym" = "bluesteel",
                             "#homeworkout" = "tomato"))
bing_together
```

```{r plot_home, eval=True, include=True}
#
# we did not included those graphs in the final presentation since we have the grid graph illustrating both
# there are included here for your reference
plot_bing_words(bing_home_word_counts, "Most common Positive and Negative words\nin tweets on #homeworkout")
```

```{r plot_gym, eval=True, include=True}
#
# we did not included those graphs in the final presentation since we have the grid graph illustrating both
# there are included here for your reference
plot_bing_words(bing_gym_word_counts, "Most common Positive and Negative words\nin tweets on #gym")
```

## Sentiment Scores
```{r calculating_sentiment_scores, echo=FALSE, message=FALSE}
#
# using the sentiment_score function we made to create a df with the sentiment score
#
homeworkout_sentiment <- sentiment_score(hash_homeworkout_tweets_clean)
gym_sentiment <- sentiment_score(hash_gym_tweets_clean)
# Add a variable to indicate the tweet
#
homeworkout_sentiment <- homeworkout_sentiment %>% 
  mutate(tweet = "#homeworkout")
#
gym_sentiment <- gym_sentiment %>% 
  mutate(tweet = "#gym")
#
# using the sentiment_stats function we made to calculate the mean and median 
# sentiment scores of #homeworkout and #gym
homeworkout_sentiment_mean <- sentiment_stats(homeworkout_sentiment)[1]
homeworkout_sentiment_median <-  sentiment_stats(homeworkout_sentiment)[2]
gym_sentiment_mean <- sentiment_stats(gym_sentiment)[1]
gym_sentiment_median <- sentiment_stats(gym_sentiment)[2]
#
# Work out the means for each topic
# so that these can be added to the graph for each topic
# as a line and as a numerical value
#
sentiments_bind <- rbind(homeworkout_sentiment, gym_sentiment)
#
sentiment_stats_both <- sentiments_bind %>% 
  group_by(tweet) %>% 
  summarize(mean_score = mean(score),
            median_score = median(score)) 
#
# Perform the plot
#
ggplot(sentiments_bind, 
       aes(x = score, # Sentiment score on x-axis
           fill = tweet)) + # Fill bars with a color according to the topic
  geom_bar() + # geom_bar will do the tabulation for you :-)
  geom_vline(aes(xintercept = mean_score), 
             data = sentiment_stats_both) +
  # Add a vertical line at the mean scores, calculated and stored in sentiment_mean_both above
  geom_text(aes(x = mean_score, 
                y = Inf, 
                label = signif(mean_score, 3)), #decimal points
            vjust = 2, 
            data = sentiment_stats_both) + 
  geom_vline(aes(xintercept = median_score), 
             data = sentiment_stats_both) +
  # Add the mean as a number; vjust moves it down from the top of the plot
  scale_x_continuous(breaks = -15:15, 
                     minor_breaks = NULL) + # Show integers; set this to a suitably large range
  scale_fill_manual(values = c("#homeworkout" = "tomato", 
                               "#gym" = "steelblue")) + # Specify your own colours
  labs(x = "Sentiment Score" , 
       y = "Number of tweets", 
       fill = "Tweet",
       title = "Distribution of sentiment scores for #homeworkout and #gym",
       caption = 'Data Source: Twitter (derived using rtweet)') +
  facet_grid(tweet ~ .) +
  theme(legend.position = "none",  
        legend.text = element_text(size = 11),
        axis.text = element_text(size = 16, color = "black"),
        axis.title = element_text(size = 16, color = "black"),
        plot.title = element_text(size = 17, hjust = 0, face = 'bold'),
        plot.caption = element_text(size = 11, face = "italic"),
        legend.title = element_text(size = 16, face = 'bold'))


```

## Summary Statistics
```{r summary_stats_of_sentient_score, echo=FALSE, message=FALSE, comment=' '}
#
library(knitr)
# summary statistics
#
sentiments_stats_table <- sentiments_bind %>%
        group_by(tweet) %>%
        sentiment_stats()



#we use the kable() function to format our table, aling sets the columns content into center for eatch column ("c"),
kable(sentiments_stats_table,
     digits = 2, # we limit the amount of decimal digits to 2
     align = 'ccccc', # we algn each columns content to the center - "c" = center
     col.names = c("Tweet", "Mean", "Median", "Variance", "Sd", "IQR")) # we set our coloumn names

```

## Statistical Test
```{r density_plots, echo=FALSE}
# plotting density plots to have a better visualization of our disibutions
h_mean <- as.numeric(homeworkout_sentiment_mean)
h_median <- as.numeric(homeworkout_sentiment_median)
g_mean <- as.numeric(gym_sentiment_mean)
g_median <- as.numeric(gym_sentiment_median)

p1 <- ggplot(sentiments_bind, aes(x=score, color= tweet, fill = tweet))+
  geom_density(alpha = 0.2) +
  scale_fill_manual(values = c("#homeworkout" = "tomato", 
                               "#gym" = "steelblue")) +
  scale_colour_manual(values = c("#homeworkout" = "tomato", 
                               "#gym" = "steelblue")) +
   scale_x_continuous(breaks = -15:15, 
                     minor_breaks = NULL)

p + geom_vline(aes(xintercept=h_mean),
               color="tomato",linetype="dashed", size=0.6) +
    geom_text(aes(x = h_mean),
                y = 0.2,
                label = paste('Mh=',signif(h_mean, 3), sep = ''), color="tomato")+
    geom_vline(aes(xintercept=g_mean),
               color="steelblue",linetype="twodash", size=0.6) +
    geom_text(aes(x = g_mean),
                y = 0.1,
                label = paste('Mg=',signif(g_mean, 3), sep = ''), color="steelblue")

```


```{r visual_inspection_box_plot, echo=FALSE, eval=FALSE}
# t-test follows some assumptions:
# 1. scale of measurement,   # yes
# 2. simple random sampling, # not all users are the same expressful (some of them do not post their opinion), 
# and also not all the gym/homeworkout users use twitter, also we included only english tweets
# 3. normality of data distribution, # check with the shapiro.test and histogramms (gym might be but homeworkout appear bimodal)
# 4. adequacy of sample size # yes more than 1000 tweets
# 5. equality of variance in standard deviation. # Variance of #homewkorkout is 2.02 while for gym is 3.11

ggplot(sentiments_bind, aes(x = tweet, y = score, fill = tweet)) + 
  geom_boxplot(varwidth = TRUE) +
  labs(title = "Box plots of sentiment scores",
       x = NULL, 
       y = "Sentiment Score",
       fill = 'Tweet',
       caption = 'Data Source: Twitter (derived using rtweet)') +
  scale_fill_manual(values = c("#homeworkout" = "tomato", 
                               "#gym" = "steelblue")) +
  theme(legend.text = element_text(size = 11),
        axis.text = element_text(size = 16, color = "black"),
        axis.title = element_text(size = 16, color = "black"),
        plot.title = element_text(size = 17, hjust = 0, face = 'bold'),
        plot.caption = element_text(size = 11, face = "italic"),
        legend.title = element_text(size = 16, face = 'bold'))
```

## shapiro and wicox
```{r tests, echo=FALSE}
# shapirto-wilk's method
# null hypothesis of these tests is that "sample distribution is normal". 
# If the test is significant, the distribution is non-normal.

# H0 = distribution is normal
# H1 = distribution is not normal
s <- shapiro.test(homeworkout_sentiment$score)
# p-value < 0.05, the distribution is not normal
shapiro.test(gym_sentiment$score)
shapiro.test(homeworkout_sentiment$score)
# p-value < 0.05, the distribution is not normal
#
# Since the distribution is not normal we should use the  Mann-Whitney-Wilcoxon test.
# H0, the distributions of both populations are equal
# H1, is that the mean ranks are not equal.
wilcox.test(score ~ tweet, data=sentiments_bind)
# p-value < 0.05 
#The p-value = 3.6 x 10^-6 < 0.5. Therefore, at the 5% significance level, we reject the null hypothesis, and we #conclude that the sentiment scores between tweets with #gym and #homeworkout are significant different.
#
```
